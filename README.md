<a href="https://www.kaggle.com/code/alibahadorani/bloom-1b7-summarization-persian-news"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>
<a href="https://huggingface.co/ali619/bigscience-bloom-1b7-finetune-Summarization-Persian-News"><img src="https://img.shields.io/badge/Model_on-%F0%9F%A4%97-white" alt="Model on HF">

# Finetune bigScience-bloom-1B7 model with *Persian News Summary*

You can check all the steps in the notebook or you can run the notebook on *kaggle* if you want to test it out

* Because this model is big enough to make most of GPUs out-of-memory, I used `LoRA` and *4-bit* `Quantization`